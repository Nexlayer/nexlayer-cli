apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: simple-ml-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.22, pipelines.kubeflow.org/pipeline_compilation_time: '2025-01-29T19:48:48.019613',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "A simple ML pipeline for
      testing Kubeflow", "inputs": [{"default": "data/input.csv", "name": "data_path",
      "optional": true, "type": "String"}], "name": "Simple ML Pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.22}
spec:
  entrypoint: simple-ml-pipeline
  templates:
  - name: evaluate-model
    container:
      args: [--paths-json, '{{inputs.parameters.train-model-Output}}', '----output-paths',
        /tmp/outputs/Output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def evaluate_model(paths_json):
            import json
            from sklearn.metrics import accuracy_score
            import joblib
            import pandas as pd

            # Parse paths
            paths = json.loads(paths_json)
            model_path = paths['model_path']
            x_test_path = paths['x_test_path']
            y_test_path = paths['y_test_path']

            # Load model and test data
            model = joblib.load(model_path)
            X_test = pd.read_csv(x_test_path)
            y_test = pd.read_csv(y_test_path)

            # Make predictions
            y_pred = model.predict(X_test)

            # Calculate accuracy
            accuracy = accuracy_score(y_test.iloc[:, 0], y_pred)
            print(f"Model accuracy: {accuracy}")

            return accuracy

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(
                    str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Evaluate model', description='')
        _parser.add_argument("--paths-json", dest="paths_json", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = evaluate_model(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: python:3.7
    inputs:
      parameters:
      - {name: train-model-Output}
    outputs:
      artifacts:
      - {name: evaluate-model-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--paths-json", {"inputValue": "paths_json"}, "----output-paths",
          {"outputPath": "Output"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def evaluate_model(paths_json):\n    import json\n    from sklearn.metrics
          import accuracy_score\n    import joblib\n    import pandas as pd\n\n    #
          Parse paths\n    paths = json.loads(paths_json)\n    model_path = paths[''model_path'']\n    x_test_path
          = paths[''x_test_path'']\n    y_test_path = paths[''y_test_path'']\n\n    #
          Load model and test data\n    model = joblib.load(model_path)\n    X_test
          = pd.read_csv(x_test_path)\n    y_test = pd.read_csv(y_test_path)\n\n    #
          Make predictions\n    y_pred = model.predict(X_test)\n\n    # Calculate
          accuracy\n    accuracy = accuracy_score(y_test.iloc[:, 0], y_pred)\n    print(f\"Model
          accuracy: {accuracy}\")\n\n    return accuracy\n\ndef _serialize_float(float_value:
          float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n    if
          not isinstance(float_value, (float, int)):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of float.''.format(\n            str(float_value),
          str(type(float_value))))\n    return str(float_value)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Evaluate model'', description='''')\n_parser.add_argument(\"--paths-json\",
          dest=\"paths_json\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = evaluate_model(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "python:3.7"}}, "inputs": [{"name": "paths_json", "type": "String"}],
          "name": "Evaluate model", "outputs": [{"name": "Output", "type": "Float"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"paths_json":
          "{{inputs.parameters.train-model-Output}}"}'}
  - name: preprocess-data
    container:
      args: [--data-path, '{{inputs.parameters.data_path}}', '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def preprocess_data(data_path):
            import pandas as pd
            import numpy as np
            import os

            # Simulate data preprocessing
            print(f"Loading data from {data_path}")
            # Create dummy data
            data = pd.DataFrame({
                'feature1': np.random.rand(1000),
                'feature2': np.random.rand(1000),
                'target': np.random.randint(0, 2, 1000)
            })

            # Save processed data
            os.makedirs('/tmp/data', exist_ok=True)
            output_path = '/tmp/data/processed.csv'
            data.to_csv(output_path, index=False)
            print(f"Saved processed data to {output_path}")
            return output_path

        def _serialize_str(str_value: str) -> str:
            if not isinstance(str_value, str):
                raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                    str(str_value), str(type(str_value))))
            return str_value

        import argparse
        _parser = argparse.ArgumentParser(prog='Preprocess data', description='')
        _parser.add_argument("--data-path", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = preprocess_data(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: python:3.7
    inputs:
      parameters:
      - {name: data_path}
    outputs:
      parameters:
      - name: preprocess-data-Output
        valueFrom: {path: /tmp/outputs/Output/data}
      artifacts:
      - {name: preprocess-data-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--data-path", {"inputValue": "data_path"}, "----output-paths",
          {"outputPath": "Output"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def preprocess_data(data_path):\n    import pandas as pd\n    import numpy
          as np\n    import os\n\n    # Simulate data preprocessing\n    print(f\"Loading
          data from {data_path}\")\n    # Create dummy data\n    data = pd.DataFrame({\n        ''feature1'':
          np.random.rand(1000),\n        ''feature2'': np.random.rand(1000),\n        ''target'':
          np.random.randint(0, 2, 1000)\n    })\n\n    # Save processed data\n    os.makedirs(''/tmp/data'',
          exist_ok=True)\n    output_path = ''/tmp/data/processed.csv''\n    data.to_csv(output_path,
          index=False)\n    print(f\"Saved processed data to {output_path}\")\n    return
          output_path\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,
          str):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          str.''.format(\n            str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Preprocess
          data'', description='''')\n_parser.add_argument(\"--data-path\", dest=\"data_path\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = preprocess_data(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "python:3.7"}}, "inputs": [{"name": "data_path", "type": "String"}],
          "name": "Preprocess data", "outputs": [{"name": "Output", "type": "String"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"data_path":
          "{{inputs.parameters.data_path}}"}'}
  - name: simple-ml-pipeline
    inputs:
      parameters:
      - {name: data_path}
    dag:
      tasks:
      - name: evaluate-model
        template: evaluate-model
        dependencies: [train-model]
        arguments:
          parameters:
          - {name: train-model-Output, value: '{{tasks.train-model.outputs.parameters.train-model-Output}}'}
      - name: preprocess-data
        template: preprocess-data
        arguments:
          parameters:
          - {name: data_path, value: '{{inputs.parameters.data_path}}'}
      - name: train-model
        template: train-model
        dependencies: [preprocess-data]
        arguments:
          parameters:
          - {name: preprocess-data-Output, value: '{{tasks.preprocess-data.outputs.parameters.preprocess-data-Output}}'}
  - name: train-model
    container:
      args: [--data-path, '{{inputs.parameters.preprocess-data-Output}}', '----output-paths',
        /tmp/outputs/Output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def train_model(data_path):
            import pandas as pd
            from sklearn.model_selection import train_test_split
            from sklearn.ensemble import RandomForestClassifier
            import joblib
            import os
            import json

            # Load data
            data = pd.read_csv(data_path)
            X = data[['feature1', 'feature2']]
            y = data['target']

            # Split data
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

            # Train model
            model = RandomForestClassifier()
            model.fit(X_train, y_train)

            # Create output directories
            os.makedirs('/tmp/model', exist_ok=True)
            os.makedirs('/tmp/test_data', exist_ok=True)

            # Save outputs
            model_path = '/tmp/model/model.joblib'
            x_test_path = '/tmp/test_data/x_test.csv'
            y_test_path = '/tmp/test_data/y_test.csv'

            joblib.dump(model, model_path)
            X_test.to_csv(x_test_path, index=False)
            y_test.to_csv(y_test_path, index=False)

            # Return paths as JSON
            output_paths = {
                'model_path': model_path,
                'x_test_path': x_test_path,
                'y_test_path': y_test_path
            }
            return json.dumps(output_paths)

        def _serialize_str(str_value: str) -> str:
            if not isinstance(str_value, str):
                raise TypeError('Value "{}" has type "{}" instead of str.'.format(
                    str(str_value), str(type(str_value))))
            return str_value

        import argparse
        _parser = argparse.ArgumentParser(prog='Train model', description='')
        _parser.add_argument("--data-path", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = train_model(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: python:3.7
    inputs:
      parameters:
      - {name: preprocess-data-Output}
    outputs:
      parameters:
      - name: train-model-Output
        valueFrom: {path: /tmp/outputs/Output/data}
      artifacts:
      - {name: train-model-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--data-path", {"inputValue": "data_path"}, "----output-paths",
          {"outputPath": "Output"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def train_model(data_path):\n    import pandas as pd\n    from sklearn.model_selection
          import train_test_split\n    from sklearn.ensemble import RandomForestClassifier\n    import
          joblib\n    import os\n    import json\n\n    # Load data\n    data = pd.read_csv(data_path)\n    X
          = data[[''feature1'', ''feature2'']]\n    y = data[''target'']\n\n    #
          Split data\n    X_train, X_test, y_train, y_test = train_test_split(X, y,
          test_size=0.2)\n\n    # Train model\n    model = RandomForestClassifier()\n    model.fit(X_train,
          y_train)\n\n    # Create output directories\n    os.makedirs(''/tmp/model'',
          exist_ok=True)\n    os.makedirs(''/tmp/test_data'', exist_ok=True)\n\n    #
          Save outputs\n    model_path = ''/tmp/model/model.joblib''\n    x_test_path
          = ''/tmp/test_data/x_test.csv''\n    y_test_path = ''/tmp/test_data/y_test.csv''\n\n    joblib.dump(model,
          model_path)\n    X_test.to_csv(x_test_path, index=False)\n    y_test.to_csv(y_test_path,
          index=False)\n\n    # Return paths as JSON\n    output_paths = {\n        ''model_path'':
          model_path,\n        ''x_test_path'': x_test_path,\n        ''y_test_path'':
          y_test_path\n    }\n    return json.dumps(output_paths)\n\ndef _serialize_str(str_value:
          str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of str.''.format(\n            str(str_value),
          str(type(str_value))))\n    return str_value\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Train model'', description='''')\n_parser.add_argument(\"--data-path\",
          dest=\"data_path\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = train_model(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "python:3.7"}}, "inputs": [{"name": "data_path", "type": "String"}],
          "name": "Train model", "outputs": [{"name": "Output", "type": "String"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"data_path":
          "{{inputs.parameters.preprocess-data-Output}}"}'}
  arguments:
    parameters:
    - {name: data_path, value: data/input.csv}
  serviceAccountName: pipeline-runner
